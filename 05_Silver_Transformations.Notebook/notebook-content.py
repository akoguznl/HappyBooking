# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "eafed1ea-663b-4dce-bb74-1e8420473b01",
# META       "default_lakehouse_name": "HappyBooking_Lakehouse",
# META       "default_lakehouse_workspace_id": "4469d25c-bf26-4abf-a3d9-12b6ba355076",
# META       "known_lakehouses": [
# META         {
# META           "id": "eafed1ea-663b-4dce-bb74-1e8420473b01"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

from pyspark.sql.functions import col, to_date, current_timestamp, lit

# 1. Bronze tablolarÄ±nÄ± oku
df_batch = spark.read.table("bronze_hotel_bookings")
df_api = spark.read.table("bronze_currency_api") # EÄŸer booking verisi iÃ§eriyorsa
df_stream = spark.read.table("bronze_streaming_bookings")

# 2. Kaynak belirtmek iÃ§in 'source' sÃ¼tunu ekleyelim (Opsiyonel ama iyi bir pratik)
df_batch = df_batch.withColumn("data_source", lit("Batch_CSV"))
df_stream = df_stream.withColumn("data_source", lit("Streaming_Docker"))

# 3. TablolarÄ± alt alta birleÅŸtir (Union)
# Not: SÃ¼tun isimlerinin tam eÅŸleÅŸtiÄŸinden emin oluyoruz
df_combined = df_batch.unionByName(df_stream, allowMissingColumns=True)

# 4. MÃ¼kerrer KayÄ±tlarÄ± Temizle (Deduplication)
# booking_id Ã¼zerinden tekrar edenleri silip en gÃ¼ncelini bÄ±rakalÄ±m
df_cleaned = df_combined.dropDuplicates(["booking_id"])

# 5. Veri Tiplerini StandartlaÅŸtÄ±r
df_silver = df_cleaned.withColumn("checkin_date", to_date(col("checkin_date"))) \
                      .withColumn("checkout_date", to_date(col("checkout_date"))) \
                      .withColumn("silver_load_at", current_timestamp())

# 6. Silver Tablosuna Kaydet (Delta FormatÄ±nda)
df_silver.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("silver_bookings")

print("âœ… Silver Tablosu BaÅŸarÄ±yla OluÅŸturuldu!")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

from pyspark.sql.functions import col, to_date, current_timestamp, lit

# 1. Kaynak TablolarÄ± YÃ¼kle
print("ğŸ“¥ Bronze tablolarÄ± okunuyor...")
df_batch = spark.read.table("bronze_hotel_bookings")
df_stream = spark.read.table("bronze_streaming_bookings")

# 2. TablolarÄ± BirleÅŸtir (Union)
# allowMissingColumns=True sayesinde sÃ¼tun sayÄ±larÄ± farklÄ± olsa bile hata almayÄ±z
df_combined = df_batch.unionByName(df_stream, allowMissingColumns=True)

# 3. VERÄ° KALÄ°TESÄ° KONTROLLERÄ° (Data Quality)
print("ğŸ” Veri kalitesi kontrolleri baÅŸlatÄ±lÄ±yor...")

# A. Null KontrolÃ¼: Kritik alanlar boÅŸ mu?
null_count = df_combined.filter(col("booking_id").isNull()).count()
if null_count > 0:
    print(f"âš ï¸ UYARI: {null_count} adet satÄ±rda booking_id boÅŸ!")

# B. MantÄ±ksal Kontrol: Fiyat negatif olabilir mi?
negative_prices = df_combined.filter(col("total_price") < 0).count()
if negative_prices > 0:
    print(f"âŒ HATA: {negative_prices} satÄ±rda negatif fiyat tespit edildi!")

# C. Tarih KontrolÃ¼: GiriÅŸ tarihi Ã§Ä±kÄ±ÅŸ tarihinden sonra olamaz
# (Not: SÃ¼tun isimlerinizin checkin_date ve checkout_date olduÄŸunu varsayÄ±yoruz)
date_error = df_combined.filter(col("checkin_date") > col("checkout_date")).count()
if date_error > 0:
    print(f"âŒ HATA: {date_error} satÄ±rda giriÅŸ/Ã§Ä±kÄ±ÅŸ tarihi Ã§akÄ±ÅŸmasÄ± var!")

# 4. TEMÄ°ZLÄ°K VE TRANSFORMASYON
# Sadece kalite kontrolÃ¼nden geÃ§en verileri alÄ±yoruz
df_silver = df_combined.filter(
    (col("booking_id").isNotNull()) & 
    (col("total_price") >= 0)
).dropDuplicates(["booking_id"]) # MÃ¼kerrer kayÄ±tlarÄ± temizle

# Tarih formatlarÄ±nÄ± dÃ¼zelt ve yÃ¼kleme zamanÄ± ekle
df_silver_final = df_silver.withColumn("checkin_date", to_date(col("checkin_date"))) \
                           .withColumn("checkout_date", to_date(col("checkout_date"))) \
                           .withColumn("silver_load_at", current_timestamp())

# 5. Silver Tablosuna Kaydet
print("ğŸ’¾ Silver tablosu gÃ¼ncelleniyor...")
df_silver_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("silver_bookings")

print("âœ… Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±! 'silver_bookings' tablosu hazÄ±r.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
